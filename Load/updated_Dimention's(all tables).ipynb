{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21889e9-b358-4a2a-8efa-6150d335f5a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  dimtable\n======================================================================\n\n creat dim_date...\n dim_date created: 462 rows\n+--------+----------+----+-------+-----+----------+---+-----------+---------+----------+\n|date_key|      date|year|quarter|month|month_name|day|day_of_week| day_name|is_weekend|\n+--------+----------+----+-------+-----+----------+---+-----------+---------+----------+\n|20130201|2013-02-01|2013|      1|    2|  February|  1|          6|   Friday|     false|\n|20130301|2013-03-01|2013|      1|    3|     March|  1|          6|   Friday|     false|\n|20130401|2013-04-01|2013|      2|    4|     April|  1|          2|   Monday|     false|\n|20130501|2013-05-01|2013|      2|    5|       May|  1|          4|Wednesday|     false|\n|20130601|2013-06-01|2013|      2|    6|      June|  1|          7| Saturday|      true|\n+--------+----------+----+-------+-----+----------+---+-----------+---------+----------+\nonly showing top 5 rows\n\n Creatig dim_time...\n dim_time created: 24 rows\n+--------+----+------+----------+-----------------+------------+\n|time_key|hour|minute|hour_group|is_business_hours|is_rush_hour|\n+--------+----+------+----------+-----------------+------------+\n|       0|   0|     0|     Night|            false|       false|\n|     100|   1|     0|     Night|            false|       false|\n|     200|   2|     0|     Night|            false|       false|\n|     300|   3|     0|     Night|            false|       false|\n|     400|   4|     0|     Night|            false|       false|\n|     500|   5|     0|     Night|            false|       false|\n|     600|   6|     0|   Morning|            false|       false|\n|     700|   7|     0|   Morning|            false|        true|\n+--------+----+------+----------+-----------------+------------+\nonly showing top 8 rows\n\n Creating dim_rider...\n dim_rider created: 74,999 rows\n\n Creating dim_station...\n dim_station created: 837 rows\n\n ALL DIMENSION TABLES CREATED!\n======================================================================\n"
     ]
    }
   ],
   "source": [
    "###dim_date###\n",
    "\n",
    "print(\"  dimtable\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#  dim_date \n",
    "print(\"\\n creat dim_date...\")\n",
    "\n",
    "# date range from both trips and payments\n",
    "trips_dates = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT date(start_time) as trip_date \n",
    "    FROM bronze_trips\n",
    "    WHERE start_time IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "payment_dates = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT payment_date \n",
    "    FROM bronze_payments\n",
    "    WHERE payment_date IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Combine and get full date range\n",
    "all_dates = trips_dates.select(col(\"trip_date\").alias(\"date\")) \\\n",
    "    .union(payment_dates.select(col(\"payment_date\").alias(\"date\"))) \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"date\")\n",
    "\n",
    "# Add date dimension attributes\n",
    "dim_date = all_dates.select(\n",
    "    # Primary key: YYYYMMDD format\n",
    "    date_format(col(\"date\"), \"yyyyMMdd\").cast(\"int\").alias(\"date_key\"),\n",
    "    col(\"date\"),\n",
    "    year(col(\"date\")).alias(\"year\"),\n",
    "    quarter(col(\"date\")).alias(\"quarter\"),\n",
    "    month(col(\"date\")).alias(\"month\"),\n",
    "    date_format(col(\"date\"), \"MMMM\").alias(\"month_name\"),\n",
    "    dayofmonth(col(\"date\")).alias(\"day\"),\n",
    "    dayofweek(col(\"date\")).alias(\"day_of_week\"),\n",
    "    date_format(col(\"date\"), \"EEEE\").alias(\"day_name\"),\n",
    "    when(dayofweek(col(\"date\")).isin([1, 7]), True).otherwise(False).alias(\"is_weekend\")\n",
    ")\n",
    "\n",
    "# Save as Delta table\n",
    "dim_date.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold_dim_date\")\n",
    "\n",
    "print(f\" dim_date created: {dim_date.count():,} rows\")\n",
    "dim_date.show(5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###dim_time###\n",
    "\n",
    "print(\"\\n Creatig dim_time...\")\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Generate all possible hours and minutes (24 * 60 = 1440 combinations)\n",
    "# For simplicity, we'll create hourly buckets\n",
    "time_data = []\n",
    "for hour in range(24):\n",
    "    time_key = hour * 100  # 0, 100, 200, ..., 2300\n",
    "    \n",
    "    # Determine hour group\n",
    "    if 6 <= hour < 12:\n",
    "        hour_group = \"Morning\"\n",
    "    elif 12 <= hour < 18:\n",
    "        hour_group = \"Afternoon\" \n",
    "    elif 18 <= hour < 22:\n",
    "        hour_group = \"Evening\"\n",
    "    else:\n",
    "        hour_group = \"Night\"\n",
    "    \n",
    "    # Business hours flag\n",
    "    is_business_hours = 9 <= hour < 17\n",
    "    \n",
    "    # Rush hour flag (7-9 AM, 5-7 PM)\n",
    "    is_rush_hour = (7 <= hour < 9) or (17 <= hour < 19)\n",
    "    \n",
    "    time_data.append(Row(\n",
    "        time_key=time_key,\n",
    "        hour=hour,\n",
    "        minute=0,  # Simplified to hour level\n",
    "        hour_group=hour_group,\n",
    "        is_business_hours=is_business_hours,\n",
    "        is_rush_hour=is_rush_hour\n",
    "    ))\n",
    "\n",
    "dim_time = spark.createDataFrame(time_data)\n",
    "\n",
    "dim_time.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold_dim_time\")\n",
    "\n",
    "print(f\" dim_time created: {dim_time.count()} rows\")\n",
    "dim_time.show(8)\n",
    "\n",
    "\n",
    "###dim_rider-station###\n",
    "\n",
    "print(\"\\n Creating dim_rider...\")\n",
    "\n",
    "dim_rider = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        rider_id as rider_key,\n",
    "        rider_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        CONCAT(first_name, ' ', last_name) as full_name,\n",
    "        address,\n",
    "        birthday,\n",
    "        account_start_date,\n",
    "        is_member,\n",
    "        -- Calculate age at account start\n",
    "        FLOOR(DATEDIFF(account_start_date, birthday) / 365.25) as age_at_account_start,\n",
    "        -- Calculate account tenure in years\n",
    "        ROUND(DATEDIFF(CURRENT_DATE(), account_start_date) / 365.25, 2) as account_tenure_years\n",
    "    FROM bronze_riders\n",
    "\"\"\")\n",
    "\n",
    "dim_rider.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold_dim_rider\")\n",
    "\n",
    "print(f\" dim_rider created: {dim_rider.count():,} rows\")\n",
    "\n",
    "# CREATE DIM_STATION - Station information\n",
    "print(\"\\n Creating dim_station...\")\n",
    "\n",
    "dim_station = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        station_id as station_key,\n",
    "        station_id,\n",
    "        station_name,\n",
    "        latitude,\n",
    "        longitude,\n",
    "        -- Simple area classification based on coordinates\n",
    "        CASE \n",
    "            WHEN latitude > 41.9 THEN 'North Chicago'\n",
    "            WHEN latitude < 41.8 THEN 'South Chicago'  \n",
    "            ELSE 'Central Chicago'\n",
    "        END as station_area\n",
    "    FROM bronze_stations\n",
    "\"\"\")\n",
    "\n",
    "dim_station.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"gold_dim_station\")\n",
    "\n",
    "print(f\" dim_station created: {dim_station.count()} rows\")\n",
    "\n",
    "print(\"\\n ALL DIMENSION TABLES CREATED!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "updated_Dimention's",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}